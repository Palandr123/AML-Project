{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNE ON HF DATASET WITH SYLLABI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"anordkvist/gu-course-syllabus\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# Define your custom filtering function\n",
    "def custom_filter(example):\n",
    "    fields_to_check = ['Learning outcomes', 'Course content']\n",
    "    blacklist_words = ['history', 'biology', 'music', 'art', 'philosophy', 'literature', 'psychology', 'sociology', 'economics', 'political science', 'politics']\n",
    "    whitelist_words = ['IT engineering', 'IT', 'math', 'calculus', 'differential', 'machine learning', 'deep learning', 'maths', 'computer science', 'management', 'programming', 'robotics']\n",
    "    \n",
    "    # Check if the text is in English\n",
    "    for field in fields_to_check:\n",
    "        text = example[field].lower().strip().split()\n",
    "        if detect(example[field]) != 'en':\n",
    "            return False\n",
    "        \n",
    "        # Check for blacklist words\n",
    "        for word in blacklist_words:\n",
    "            if word in text:\n",
    "                return False\n",
    "        \n",
    "        # Check for whitelist words\n",
    "        for word in whitelist_words:\n",
    "            if word in example[field]:\n",
    "                return True\n",
    "    \n",
    "    # If no whitelist words are found, exclude the example\n",
    "    return False\n",
    "\n",
    "        \n",
    "\n",
    "# Apply the filtering function to the dataset\n",
    "filtered_dataset = dataset.filter(custom_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['course_code', 'Confirmation', 'Position in the educational system', 'Entry requirements', 'Learning outcomes', 'Course content', 'Form of teaching', 'Assessment', 'Grades', 'Course evaluation', 'Additional information', '__index_level_0__'],\n",
       "    num_rows: 631\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(example):\n",
    "    s1 =  example['Learning outcomes'][example['Learning outcomes'].find(',') + 2:]\n",
    "    new_feature_value =  s1 + '\\n' + example['Course content']\n",
    "    example['text'] = new_feature_value\n",
    "    return example\n",
    "\n",
    "filtered_dataset = filtered_dataset.map(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 631\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_to_check = ['text']\n",
    "\n",
    "# Get all column names\n",
    "all_columns = filtered_dataset.column_names\n",
    "\n",
    "# Remove the columns you want to keep from the list of all columns\n",
    "columns_to_remove = [col for col in all_columns if col not in fields_to_check]\n",
    "\n",
    "# Use the .remove_columns method to exclude these columns\n",
    "dataset_processed = filtered_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'learning outcomes on successful completion of the course the student will be able to: knowledge and understanding •explain the lifecycle of data-intensive systems, starting from data creation, to validation, processing, presentation, storage, and archiving •explain the issues related to the integration of ai techniques in software systems, e.g., machine learning, data analysis, computer vision, or autonomous decision making •name and describe different common ai techniques and to which problems they are applicable •explain the impact of different data analysis goals on the required format, content, and quality of the data and the applicability of different ai techniques competence and skills •use common artificial intelligence techniques to solve pre-defined problems •apply techniques to validate and deploy data-intensive ai systems in the operational context judgement and approach •discuss the advantages and disadvantages of different patterns and architectures for data-intensive systems •discuss the principles of learning from potentially partial or low-quality data and the impact on the quality of the system •analyse and discuss the impact of design choices about the different steps in the data lifecycle on ethical issues related to the privacy and security, as well as the ethical use of data\\npassage: course code: dit822, course content this course addresses issues relevant for software engineering for systems that use artificial intelligence (ai) techniques such as machine learning or large-scale parallel data processing. the course gives (a) an introduction of basic principles of ai, with emphasis on the principles and techniques used in machine learning (ml) and deep learning (dl), and (b) insights to support needed for successful implementation of ai systems. the course addresses the life cycle of ai systems: it includes data preparation (i.e. collecting data, data processing, storage, analysis), and building ai models by training and validation. it also discusses use of data, such as implications of using different data sets for the same goal, or using the same data set for different goals. furthermore, the course discusses how software systems need to be structured and deployed in order to achieve the performance required for realistic applications. relevant software architectures and patterns are introduced and discussed in the context 2/ 4 dit822 software engineering for ai systems, 7.5 credits / software engineering för ai-system, 7,5 högskolepoäng first cycle of a realistic application scenario. finally, the ethical considerations in using data and providing automatically-created solutions are discussed. the students will learn the basic ml and dl methods, processing and analyzing data in relation to the requirements, and the goals of the system implementation. further they will learn dependencies of the results to the selected data sets including its annotation. the students will understand different data types, such as static, and streams, and different type of systems that use ai techniques. sub-courses 1.written exam (tentamen), 4.5 credits grading scale: pass with distinction (5), pass with credit (4), pass (3) and fail (u) 2.assignments (inlämningsuppgifter), 3 credits grading scale: pass (g) and fail (u)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folder(folder_path):\n",
    "    data_files = os.listdir(folder_path)\n",
    "    data = []\n",
    "    for file_name in data_files:\n",
    "        if file_name.lower().endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                item = json.load(file)\n",
    "                data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = os.path.join(\"..\", \"syllabuses\")\n",
    "data = load_data_from_folder(folder_path)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'Network And Cyber Security',\n",
       " 'Short Description': 'This course covers the following concepts: Information Security Management; Web Security; Software Security; Network security.',\n",
       " 'Prerequisites': {'Prerequisite subjects': [], 'Prerequisite topics': []},\n",
       " 'Course Topics': [{'Section': 'Information Security Management',\n",
       "   'Topics within the section': ['Security Policies and Controls',\n",
       "    'Risks Analysis and Threats Modeling',\n",
       "    'Software Development Security Techniques']},\n",
       "  {'Section': 'Web Security',\n",
       "   'Topics within the section': ['Security-related web technologies',\n",
       "    'Same Origin Policy',\n",
       "    'Web Attacker Model',\n",
       "    'OWASP methodology',\n",
       "    'Injection Flaws',\n",
       "    'Authorization Flaws',\n",
       "    'Cookies Flaws',\n",
       "    'Server Misconfiguration']},\n",
       "  {'Section': 'Software Security',\n",
       "   'Topics within the section': ['Binary Exploitation', 'ASLR', 'NX']},\n",
       "  {'Section': 'Network Security',\n",
       "   'Topics within the section': ['Networking tools',\n",
       "    'Network attacks',\n",
       "    'IDS/IPS']}],\n",
       " 'Intended Learning Outcomes (ILOs)': {'What is the main purpose of this course?': 'Security breaches cost billions of dollars worth of damage to the computing industry. Today, cybercriminals control armies consisting of several millions of compromised machines. Attacks are increasingly being perpetrated towards enterprises, individuals, critical infrastructure and even governments. At the same time, our computer systems and platforms are fast evolving to meet the demands of the industry. Increasing the use of personalized devices, and our growing dependence on legacy computer systems that weren’t designed with security in mind is a challenge ahead. Therefore, the purpose of this course is to cover the design and implementation of different IT systems from a security perspective. This course introduces to the field of systems security: that is, how to analyze and develop secure systems. The course covers fundamental concepts of systems design, low and high-level vulnerabilities exploitation, design, and implementation flaws in different types of applications based on the real-world scenarios.',\n",
       "  'ILOs defined at three levels': {'Level 1: What concepts should a student know/remember/explain?': ['Security policies and controls',\n",
       "    'Risks and threats related to the system design and its implementation',\n",
       "    'Software security testing methodologies',\n",
       "    'Software development security techniques',\n",
       "    'Injection and authorization flaws',\n",
       "    'Cookies and misconfiguration flaws',\n",
       "    'Common weaknesses/vulnerabilities in web applications',\n",
       "    'Common weaknesses/vulnerabilities in the typical systems software'],\n",
       "   'Level 2: What basic practical skills should a student be able to perform?': ['Information security management methods',\n",
       "    'Difference between different types of risks and threats',\n",
       "    'Security-related web technologies',\n",
       "    'The difference in the different web application flaws',\n",
       "    'ASLR, NX and how are these techniques can help to protect against a malicious attacker',\n",
       "    'Covert channels',\n",
       "    'Networking tools',\n",
       "    'Network proxies'],\n",
       "   'Level 3: What complex comprehensive skills should a student be able to apply in real-life scenarios?': ['Critically audit systems and code for security flaws and threats',\n",
       "    'Design and implement exploits for real security vulnerabilities',\n",
       "    'Develop secure systems and applications',\n",
       "    'Be able to design defense solutions and outline their limitations',\n",
       "    'Be able to find misconfigurations/vulnerabilities in a given network/system']}},\n",
       " 'Grading': {'Course grading range': [{'Grade': 'A. Excellent',\n",
       "    'Range': '90-100',\n",
       "    'Description of performance': '-'},\n",
       "   {'Grade': 'B. Good', 'Range': '70-89', 'Description of performance': '-'},\n",
       "   {'Grade': 'C. Satisfactory',\n",
       "    'Range': '60-69',\n",
       "    'Description of performance': '-'},\n",
       "   {'Grade': 'D. Poor', 'Range': '0-59', 'Description of performance': '-'}],\n",
       "  'Course activities and grading breakdown': [{'Activity Type': 'Labs/seminar classes',\n",
       "    'Percentage of the overall course grade': '30'},\n",
       "   {'Activity Type': 'Project',\n",
       "    'Percentage of the overall course grade': '30'},\n",
       "   {'Activity Type': 'Exams', 'Percentage of the overall course grade': '40'}],\n",
       "  'Recommendations_for_students_on_how_to_succeed_in_the_course': []},\n",
       " 'Resources, literature and reference materials': {'Open access resources': ['Mike Chapple, James Michael Stewart, Darril Gibson, CISSP Official Study Guide, 8th Edition, Sybex, 2018',\n",
       "   'Michal Zalewsk, The Tangled Web, No Starch Press, 2011',\n",
       "   'Jon Erickson, Hacking: The Art of Exploitation, 2nd Edition, No Starch Press, 2008'],\n",
       "  'Closed access resources': [],\n",
       "  'Software and tools used within the course': []},\n",
       " 'Activities and Teaching Methods': [{'Learning Activities': 'Homework and group projects',\n",
       "   'Section 1': '1',\n",
       "   'Section 2': '1',\n",
       "   'Section 3': '1',\n",
       "   'Section 4': '1'},\n",
       "  {'Learning Activities': 'Testing (written or computer based)',\n",
       "   'Section 1': '1',\n",
       "   'Section 2': '1',\n",
       "   'Section 3': '1',\n",
       "   'Section 4': '1'},\n",
       "  {'Learning Activities': 'Reports',\n",
       "   'Section 1': '1',\n",
       "   'Section 2': '1',\n",
       "   'Section 3': '1',\n",
       "   'Section 4': '1'},\n",
       "  {'Learning Activities': 'Discussions',\n",
       "   'Section 1': '1',\n",
       "   'Section 2': '1',\n",
       "   'Section 3': '1',\n",
       "   'Section 4': '1'},\n",
       "  {'Learning Activities': 'Development of individual parts of software product code',\n",
       "   'Section 1': '0',\n",
       "   'Section 2': '1',\n",
       "   'Section 3': '0',\n",
       "   'Section 4': '0'},\n",
       "  {'Learning Activities': 'Midterm evaluation',\n",
       "   'Section 1': '0',\n",
       "   'Section 2': '1',\n",
       "   'Section 3': '0',\n",
       "   'Section 4': '0'}],\n",
       " 'Formative Assessment and Course Activities': {'Ongoing performance assessment': {'Section 1': [{'Activity Type': 'Question',\n",
       "     'Content': 'What types of Security Policies are exist?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'What information from a given system you need to take into account to calculate security risks?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Explain the difference between static and dynamic analysis of application code?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Audit the given security policy for vulnerabilities and update it accordingly',\n",
       "     'Is Graded?': '0'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Calculate security risks for a given system and develop necessary security measures for mitigation',\n",
       "     'Is Graded?': '0'}],\n",
       "   'Section 2': [{'Activity Type': 'Question',\n",
       "     'Content': 'What is the difference between reflected XSS and stored XSS? which one is more critical and why?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'What are the pros and cons of using regex to protect against XSS?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'what is the Same Origin Policy? and which attack does it mitigate?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'What is the difference between boolean-based and time-based SQL injection?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Vulnerability analysis and exploitation for a given web application',\n",
       "     'Is Graded?': '0'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Write and deploy WAF rules to mitigate a specific web attack',\n",
       "     'Is Graded?': '0'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Does the Same Origin Policy apply to the localStorage inside the browser?',\n",
       "     'Is Graded?': '0'}],\n",
       "   'Section 3': [{'Activity Type': 'Question',\n",
       "     'Content': 'What are the pros and cons of using ASLR? does it affect the performance?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'What can you do with a format string vulnerability?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'What is the required information to be able to identify a remote libc version?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Why some binaries might have the same address for their functions? what is the security risk of this?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Vulnerability analysis and exploitation for a given binary while ASLR is disabled',\n",
       "     'Is Graded?': '0'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Try to rewrite the following Assembly code in any programming language',\n",
       "     'Is Graded?': '0'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'How can you check if you have ASLR, PIE, NX enabled or not?',\n",
       "     'Is Graded?': '0'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Decompilers are not always accurate why? how can you improve it?',\n",
       "     'Is Graded?': '0'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Some binaries are shipped with debugging symbols, How can this help you in debugging?',\n",
       "     'Is Graded?': '0'}],\n",
       "   'Section 4': [{'Activity Type': 'Question',\n",
       "     'Content': 'What is the difference between VPN and sock5?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'What are IDS, IPS, and DPI?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'Why does Nmap produce false-positive when scanning a windows host? can you improve the scanning technique?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'What is covert channel? what are the most common protocols that are used for covert channel? why?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'When using a proxy for HTTPS, your browser will always complain about the certificate, how can you solve this issue?',\n",
       "     'Is Graded?': '1'},\n",
       "    {'Activity Type': 'Question',\n",
       "     'Content': 'No lab for this section',\n",
       "     'Is Graded?': '0'}]},\n",
       "  'Final assessment': {'Section 1': ['As above'],\n",
       "   'Section 2': ['As above'],\n",
       "   'Section 3': ['As above'],\n",
       "   'Section 4': ['As above']}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_course_data(data):\n",
    "    def process(x):\n",
    "        if (x.get('Title') and x.get('Short Description') and x.get('Course Topics') and\n",
    "            x.get('Intended Learning Outcomes (ILOs)') and x.get('Formative Assessment and Course Activities')\n",
    "           ):\n",
    "            return ('TITLE: ' + x.get('Title').strip()+'\\n'+'DESCRIPTION: '+  x.get('Short Description').strip()+\n",
    "                    '\\n'+'COURSE_TOPICS: '+str(x.get('Course Topics')) + '\\n'\n",
    "                   +'INTENDED_LEARNING_OUTCOMES: ' + str(x.get('Intended Learning Outcomes (ILOs)')) + '\\n'\n",
    "                   + 'FINAL_ASSESSMENT: ' + str(x.get('Formative Assessment and Course Activities')))\n",
    "        return None\n",
    "\n",
    "    return [process(x) for x in data if process(x)]\n",
    "\n",
    "data_proc = preprocess_course_data(data)\n",
    "dataset = Dataset.from_dict({\"text\": data_proc})\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TITLE: Network And Cyber Security\\nDESCRIPTION: This course covers the following concepts: Information Security Management; Web Security; Software Security; Network security.\\nCOURSE_TOPICS: [{'Section': 'Information Security Management', 'Topics within the section': ['Security Policies and Controls', 'Risks Analysis and Threats Modeling', 'Software Development Security Techniques']}, {'Section': 'Web Security', 'Topics within the section': ['Security-related web technologies', 'Same Origin Policy', 'Web Attacker Model', 'OWASP methodology', 'Injection Flaws', 'Authorization Flaws', 'Cookies Flaws', 'Server Misconfiguration']}, {'Section': 'Software Security', 'Topics within the section': ['Binary Exploitation', 'ASLR', 'NX']}, {'Section': 'Network Security', 'Topics within the section': ['Networking tools', 'Network attacks', 'IDS/IPS']}]\\nINTENDED_LEARNING_OUTCOMES: {'What is the main purpose of this course?': 'Security breaches cost billions of dollars worth of damage to the computing industry. Today, cybercriminals control armies consisting of several millions of compromised machines. Attacks are increasingly being perpetrated towards enterprises, individuals, critical infrastructure and even governments. At the same time, our computer systems and platforms are fast evolving to meet the demands of the industry. Increasing the use of personalized devices, and our growing dependence on legacy computer systems that weren’t designed with security in mind is a challenge ahead. Therefore, the purpose of this course is to cover the design and implementation of different IT systems from a security perspective. This course introduces to the field of systems security: that is, how to analyze and develop secure systems. The course covers fundamental concepts of systems design, low and high-level vulnerabilities exploitation, design, and implementation flaws in different types of applications based on the real-world scenarios.', 'ILOs defined at three levels': {'Level 1: What concepts should a student know/remember/explain?': ['Security policies and controls', 'Risks and threats related to the system design and its implementation', 'Software security testing methodologies', 'Software development security techniques', 'Injection and authorization flaws', 'Cookies and misconfiguration flaws', 'Common weaknesses/vulnerabilities in web applications', 'Common weaknesses/vulnerabilities in the typical systems software'], 'Level 2: What basic practical skills should a student be able to perform?': ['Information security management methods', 'Difference between different types of risks and threats', 'Security-related web technologies', 'The difference in the different web application flaws', 'ASLR, NX and how are these techniques can help to protect against a malicious attacker', 'Covert channels', 'Networking tools', 'Network proxies'], 'Level 3: What complex comprehensive skills should a student be able to apply in real-life scenarios?': ['Critically audit systems and code for security flaws and threats', 'Design and implement exploits for real security vulnerabilities', 'Develop secure systems and applications', 'Be able to design defense solutions and outline their limitations', 'Be able to find misconfigurations/vulnerabilities in a given network/system']}}\\nFINAL_ASSESSMENT: {'Ongoing performance assessment': {'Section 1': [{'Activity Type': 'Question', 'Content': 'What types of Security Policies are exist?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'What information from a given system you need to take into account to calculate security risks?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'Explain the difference between static and dynamic analysis of application code?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'Audit the given security policy for vulnerabilities and update it accordingly', 'Is Graded?': '0'}, {'Activity Type': 'Question', 'Content': 'Calculate security risks for a given system and develop necessary security measures for mitigation', 'Is Graded?': '0'}], 'Section 2': [{'Activity Type': 'Question', 'Content': 'What is the difference between reflected XSS and stored XSS? which one is more critical and why?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'What are the pros and cons of using regex to protect against XSS?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'what is the Same Origin Policy? and which attack does it mitigate?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'What is the difference between boolean-based and time-based SQL injection?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'Vulnerability analysis and exploitation for a given web application', 'Is Graded?': '0'}, {'Activity Type': 'Question', 'Content': 'Write and deploy WAF rules to mitigate a specific web attack', 'Is Graded?': '0'}, {'Activity Type': 'Question', 'Content': 'Does the Same Origin Policy apply to the localStorage inside the browser?', 'Is Graded?': '0'}], 'Section 3': [{'Activity Type': 'Question', 'Content': 'What are the pros and cons of using ASLR? does it affect the performance?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'What can you do with a format string vulnerability?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'What is the required information to be able to identify a remote libc version?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'Why some binaries might have the same address for their functions? what is the security risk of this?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'Vulnerability analysis and exploitation for a given binary while ASLR is disabled', 'Is Graded?': '0'}, {'Activity Type': 'Question', 'Content': 'Try to rewrite the following Assembly code in any programming language', 'Is Graded?': '0'}, {'Activity Type': 'Question', 'Content': 'How can you check if you have ASLR, PIE, NX enabled or not?', 'Is Graded?': '0'}, {'Activity Type': 'Question', 'Content': 'Decompilers are not always accurate why? how can you improve it?', 'Is Graded?': '0'}, {'Activity Type': 'Question', 'Content': 'Some binaries are shipped with debugging symbols, How can this help you in debugging?', 'Is Graded?': '0'}], 'Section 4': [{'Activity Type': 'Question', 'Content': 'What is the difference between VPN and sock5?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'What are IDS, IPS, and DPI?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'Why does Nmap produce false-positive when scanning a windows host? can you improve the scanning technique?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'What is covert channel? what are the most common protocols that are used for covert channel? why?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'When using a proxy for HTTPS, your browser will always complain about the certificate, how can you solve this issue?', 'Is Graded?': '1'}, {'Activity Type': 'Question', 'Content': 'No lab for this section', 'Is Graded?': '0'}]}, 'Final assessment': {'Section 1': ['As above'], 'Section 2': ['As above'], 'Section 3': ['As above'], 'Section 4': ['As above']}}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetDict = dataset.train_test_split(train_size=0.7, shuffle=True, seed=42)\n",
    "train_dataset_only_inno = datasetDict['train']\n",
    "eval_dataset = datasetDict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 17\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 670\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset = concatenate_datasets([train_dataset_only_inno])\n",
    "train_dataset = concatenate_datasets([dataset_processed, train_dataset_only_inno])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define basic prompt for comparison between baseline & finetuned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_eval(model, tokenizer):\n",
    "    model_params = {\n",
    "        \"max_new_tokens\": 512,\n",
    "    }\n",
    "    with open(\"prompt_template_finetune.txt\", \"r\", encoding='utf-8') as f:\n",
    "        prompt_base = f.read()\n",
    "    COURSE_NAME = \"Statistical Techniques For Data Science and Robotics\"\n",
    "    COURSE_DESCRIPTION = \"This advanced course covers main concepts in statics used in industry, including hypothesis testing, statistical tests, probabilitic and bayesian models.\"\n",
    "    prompt_str = f\"{prompt_base}\\nCOURSE: {COURSE_NAME}\\DESCRIPTION: {COURSE_DESCRIPTION}\\nCOURSE_TOPICS: \"\n",
    "    chat = [\n",
    "        { \"role\": \"user\", \"content\": prompt_str},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids.cuda(),\n",
    "        **model_params,\n",
    "    )\n",
    "    response = tokenizer.decode(generated_ids[0])\n",
    "    return response[len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try gemma 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['HF_TOKEN'] = '<YOUR TOKEN>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrudakov/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3225ab21554e1faf81bf3e01311192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrudakov/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"google/gemma-1.1-2b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(prompt['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample check on baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sample_eval(model, tokenizer)\n",
    "with open('gemma-2b-it-baseline-answer.txt', 'w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"Course\": \"Statistical Techniques For Data Science and Robotics\",\n",
      "\"CourseDescription\": \"This course provides an introduction to statistical techniques commonly used in data science and robotics. Topics include hypothesis testing, statistical tests, probability and Bayesian models, and their applications in data analysis and model building.\"\n",
      "\"CourseTopics\":\n",
      "[\n",
      "{\n",
      "\"Section\": \"Hypothesis Testing and Statistical Inference\",\n",
      "\"Topics\": [\"Introduction to hypothesis testing\", \"Statistical inference\", \"Hypothesis testing procedures\", \"Confidence intervals\"]\n",
      "},\n",
      "{\n",
      "\"Section\": \"Statistical Tests and Probability Theory\",\n",
      "\"Topics\": [\"Statistical tests for means\", \"Statistical tests for variances\", \"Probability distributions\", \"Bayesian inference\"]\n",
      "},\n",
      "{\n",
      "\"Section\": \"Bayesian Models and Decision Theory\",\n",
      "\"Topics\": [\"Bayesian inference\", \"Bayes' theorem\", \"Decision theory\", \"Bayesian networks\"]\n",
      "},\n",
      "{\n",
      "\"Section\": \"Advanced Topics in Statistical Techniques\",\n",
      "\"Topics\": [\"Generalized linear models\", \"Logistic regression\", \"Decision trees\", \"Ensemble methods\"]\n",
      "}\n",
      "]\n",
      "}\n",
      "```<eos>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & preparation continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b03d00195e413486c6e4bb9c88fdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_eval_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 1024 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt['text'],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0916b1ad544e40aaa22402548f055e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_eval_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 27520, 18701, 611, 8385, 17230, 576, 573, 3205, 235269, 573, 5913, 877, 614, 3326, 577, 235292, 10200, 573, 9156, 576, 573, 17482, 5168, 5449, 576, 3273, 235269, 16112, 20345, 235269, 22784, 29420, 578, 47003, 576, 6439, 57278, 18739, 3679, 774, 573, 9245, 5681, 4159, 1570, 573, 17482, 575, 17526, 235248, 235274, 708, 1671, 3104, 8557, 3482, 611, 573, 5358, 575, 17526, 235248, 235274, 235265, 108, 135945, 235292, 3205, 3409, 235292, 40314, 235274, 235276, 235304, 235310, 235269, 3205, 3381, 736, 3205, 603, 671, 15683, 577, 3757, 2845, 16254, 575, 22784, 4815, 235292, 235248, 235274, 235275, 573, 5168, 5449, 576, 3273, 578, 573, 30567, 576, 8199, 235290, 59050, 21975, 235265, 692, 877, 3918, 1368, 577, 14461, 573, 5168, 5449, 7949, 576, 573, 134901, 235290, 15048, 32640, 235290, 2054, 2091, 578, 577, 1281, 573, 11995, 7949, 575, 202075, 235265, 235248, 235284, 235275, 16112, 20345, 235265, 783, 877, 6652, 573, 2517, 1166, 10513, 577, 573, 47003, 576, 60952, 20345, 235269, 578, 1281, 573, 33725, 17827, 70775, 577, 42877, 23505, 675, 13040, 235265, 235248, 235304, 235275, 22784, 578, 14072, 29420, 1185, 573, 3811, 919, 5162, 12568, 578, 476, 21759, 1913, 235265, 783, 877, 3918, 1368, 577, 11560, 573, 10588, 235349, 235256, 29420, 3210, 142493, 235265, 235248, 235310, 235275, 47003, 576, 6439, 877, 6045, 611, 1368, 577, 5598, 476, 6790, 3679, 3484, 235289, 783, 877, 1281, 3679, 774, 573, 9245, 5681, 4159, 685, 476, 4303, 576, 19486, 235265, 577, 7433, 674, 573, 6045, 576, 573, 3205, 603, 611, 573, 97551, 576, 1450, 5004, 235269, 573, 25806, 1702, 576, 573, 3205, 877, 614, 23519, 577, 17355, 21426, 578, 2270, 6438, 235265, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5935104 || all params: 1521203200 || trainable%: 0.3901585271448285\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GemmaForCausalLM(\n",
      "      (model): GemmaModel(\n",
      "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-17): 18 x GemmaDecoderLayer(\n",
      "            (self_attn): GemmaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): GemmaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): GemmaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=16384, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): GemmaRMSNorm()\n",
      "            (post_attention_layernorm): GemmaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): GemmaRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=2048, out_features=256000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=2048, out_features=4, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=4, out_features=256000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jq0nX33BmfaC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/mrudakov/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=2.285608005523682, metrics={'train_runtime': 67.8251, 'train_samples_per_second': 2.949, 'train_steps_per_second': 2.949, 'total_flos': 2442632626176000.0, 'train_loss': 2.285608005523682, 'epoch': 0.29850746268656714})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = os.path.join('.', 'output')\n",
    "os.environ['WANDB_PROJECT'] = 'syllabus-finetune'\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=30,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=200,\n",
    "        learning_rate=2e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        # save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        # save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=250,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=False,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        # run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e605b835404a0581d0ce410fa5d825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▁▂▃▄▅▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▁▂▃▄▅▆▇██</td></tr><tr><td>train/grad_norm</td><td>▆▆▁▆▄█▃▄▂▃</td></tr><tr><td>train/learning_rate</td><td>████▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>2442632626176000.0</td></tr><tr><td>train/epoch</td><td>0.29851</td></tr><tr><td>train/global_step</td><td>200</td></tr><tr><td>train/grad_norm</td><td>15.74681</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.0853</td></tr><tr><td>train_loss</td><td>2.28561</td></tr><tr><td>train_runtime</td><td>67.8251</td></tr><tr><td>train_samples_per_second</td><td>2.949</td></tr><tr><td>train_steps_per_second</td><td>2.949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sky-28</strong> at: <a href='https://wandb.ai/glemhel/syllabus-finetune/runs/z1ugo926' target=\"_blank\">https://wandb.ai/glemhel/syllabus-finetune/runs/z1ugo926</a><br/> View project at: <a href='https://wandb.ai/glemhel/syllabus-finetune' target=\"_blank\">https://wandb.ai/glemhel/syllabus-finetune</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240513_143834-z1ugo926/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/mrudakov/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.29 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msample_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma-2b-it-finetuned-answer.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(result)\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36msample_eval\u001b[0;34m(model, tokenizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(chat, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;28mlen\u001b[39m(prompt):]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1135\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1122\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1123\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1132\u001b[0m )\n\u001b[1;32m   1134\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1135\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1137\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/peft/tuners/lora/layer.py:509\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 509\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     x \u001b[38;5;241m=\u001b[39m dropout(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.29 GiB. GPU "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result = sample_eval(model, tokenizer)\n",
    "    with open('gemma-2b-it-finetuned-answer.txt', 'w') as f:\n",
    "        f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\"Course_Title\": \"Statistical Techniques for Data Science and Robotics\",\\n\"Course_Description\": \"This course covers main concepts in statics used in industry, including hypothesis testing, statistical tests, probability and bayesian models.\",\\n\"Course_Topics\": [\\n\"Supervised Learning\",\\n\"Decision Trees and Ensemble Methods\",\\n\"Unsupervised Learning\",\\n\"Deep Learning\"\\n]\\n}\\n```<eos>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_params = {}\n",
    "for name, param in model.named_parameters():\n",
    "  if 'lora' in name:  # This condition depends on how LORA parameters are named in your model\n",
    "    adapted_params[name] = param.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(adapted_params, 'gemma-1.1-2b-it-finetune.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrudakov/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2832d57dcde747eba38ff0ef84bcf418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrudakov/.local/share/virtualenvs/AML-Project-RNdI0Dn9/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"google/gemma-1.1-2b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model2 = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm_head.lora_A.default.weight\n",
      "base_model.model.lm_head.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "# Load the saved parameters\n",
    "adapted_params = torch.load('gemma-1.1-2b-it-finetune.pth')\n",
    "\n",
    "# Assuming you have a fresh model instance\n",
    "for name, param in adapted_params.items():\n",
    "    print(name)\n",
    "    continue\n",
    "    # You might need to ensure the parameter names match exactly or adapt this logic to fit\n",
    "    target_param = dict(model2.named_parameters())[name]\n",
    "    with torch.no_grad():\n",
    "        target_param.copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
